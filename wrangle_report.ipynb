{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wragle_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REPORT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first thing I did was to gather all the data I needed in the data wrangling process. This includes the WeRateDogs Twitter Archive, the tweet image predictions and data from Twitter API. All this was made possible by the libraries imported such as numpy and pandas.\n",
    "- I performed both visual and programmatic assessments in order to get any quality and tidiness issues in the datasets.\n",
    "- From my assessment I managed to get the following Quality issues:\n",
    "  > Not all ratings are original ratings, some are retweets. Only original ratings are required and some data ratings are mostly retweets ie retweet_status_id not NaN\n",
    "\n",
    "  > Dog stages with no data represented as 'None' while other columns with no data represented as 'Nan'\n",
    "\n",
    "  > Lowercase given names and predictions\n",
    "\n",
    "  > Invalid data type for timestamps. The timestamps datatype should be of DateTime instead of object datatype.\n",
    "\n",
    "  > Dog predictions have '_' between the names eg English_springer\n",
    "\n",
    "  > Some columns eg the df_tweets have the tweet id column named as \"id\" while the twitter archive dataframe is named \"tweet_id\". \n",
    "\n",
    "  > Duplicates in the dataframe\n",
    "\n",
    "  > More than 50 columns with dog names as \"A\"\n",
    "  \n",
    "  > Source column should be dropped to tidy up the data and reduce bulkiness in the dataset\n",
    "\n",
    "- And these tidiness issue:\n",
    "    > The dog stages in different columns while there could be one column with each row having its own dog stage\n",
    "    > Merging all the datasets into  one\n",
    "- After gathering, assessing the data and noting down a few issues, the next thing was to begin cleaning the data according to the data cleaning procedure ie Define, Code and Test.\n",
    "- Before cleaning the data, I made a copy of the original data first. Most of my cleaning process was programmatic where I used code in the form of pandas library to autmate the cleaning tasks since it saves alot of time.\n",
    "- I documented each issue's cleaning procedure ie \n",
    "   > Define- Stated the actions I'd take to clean the data and fix the issue.\n",
    "   > Code - Contained the code that performed the task listed in the Define step.\n",
    "   > Test- This made sure the code worked\n",
    "   \n",
    "- I made some changes in the dataset from the cleaning process. These include:\n",
    "    > Dropped the rows containing retweet status id since these were not the original dog ratings but retweets.\n",
    "    \n",
    "    > Merged the four columns containing dogs' ratings into one column and thereafter replaced all 'None' values with np.nan.\n",
    "    \n",
    "    > The analysis contained three diffeerent datasets so I merged all these into one for tidiness purposes.\n",
    "    \n",
    "    > Changed all the lowercase given names and predictions to uppercase names.\n",
    "    \n",
    "    > Changed the datatype for timestamps from 'object' to 'Datetime' datatype.\n",
    "    \n",
    "    > Cleaned up the dog predictions with an underscore between the names and replaced it with a space character.\n",
    "    \n",
    "    > Renamed some columns to a more suitable name whose eaning can be easily identified.\n",
    "    \n",
    "    > Dropped the duplicates in the dataset.\n",
    "    \n",
    "    > Dropped some columns, example, url column because they were not needed in my analysis.\n",
    "\n",
    "- After the cleaning process, I made sure the dataset was all cleaned, with all the gathered pieces of data.\n",
    "- The final step was to save my gathered, assessed and cleaned master dataset to a CSV file\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
